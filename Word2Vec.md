# ğŸš€ My First Walk Into AI & Word Embeddings  

Let me start before I get overwhelmed by the mountain of research papers ğŸ“š related to word embeddings. I havenâ€™t read many papers yet, but I have certainly gone through the original **word2vec** paper, did some coding experiments ğŸ’», and found some fascinating results â€” like how *â€œengineerâ€* and *â€œplumberâ€* ended up surprisingly close in one dataset.  

---

## âœ¨ The Beginning of My AI Journey  

Leaving everything else aside, this is my **first time**, my first walk into the long journey of AI.  
Iâ€™m determined to make this year a year filled with **learning more and more about AI**, writing âœï¸ about it, and letting myself get excited ğŸ¤© by it.  

God, I feel the urgent need for a printer ğŸ–¨ï¸ â€” oh, I need that! I love messing up my room with printed papers that I can occasionally flip through and cite.  

---

## ğŸ¯ My Motive  

My motive with this journey is to better unravel the world of emerging AI, especially by staying close to **transformers** ğŸ¤–.  
Iâ€™m heavily impressed by this technology. I sometimes find myself thinking deeply about it â€” and I want to think even deeper, get crazy ğŸ¤¯ about it, stuff myself with more knowledge, and just write.  

I wish I could teach ğŸ§‘â€ğŸ«, but for now, itâ€™s just writing. Hopefully, someday someone will notice my work, or maybe Iâ€™ll land onto something meaningful through these efforts.  

---

## ğŸ”‘ Why Start With Word Embeddings?  

Word Embeddings â€” letâ€™s get straight into the work and be less poetic about my love for AI â¤ï¸.  

I had spent some time thinking about the best topic to kick off this chain of articles, and I couldnâ€™t find a better candidate than **word embeddings**.  

---

## ğŸŒ Words, Meanings, and Context  

I donâ€™t understand Chinese ğŸ‡¨ğŸ‡³, but over a billion and a half people do. If you were to remove language for even a second, the whole country would be thrown into confusion ğŸ˜µ.  

We use words â€” which are just random symbols ğŸ”¤ pronounced in certain ways â€” and they carry meanings depending on the context. But this is not how computers communicate.  

Computers ğŸ’» communicate purely through **numbers**, which usually represent one fixed thing: either **on** (1) or **off** (0).  

So, in order for computers to talk to us â€” to understand our language â€” they need to work with words, even though all they inherently understand are numbers.  

---

## âš¡ The Tricky Part  

Hereâ€™s where it gets tricky ğŸŒ€:  
A word **cannot** be represented by a fixed number.  

ğŸ‘‰ For example, the word *bank* in one context could mean a place to store money ğŸ’°, and in another, it could mean the side of a river ğŸŒŠ.  

This leads to the realization that **meaning depends on context**, and if we were to assign numbers to words, those numbers would need to change based on the context.  

ğŸ’¡ **First key realization:** There is no universal way to assign a word a fixed number.  

---

## ğŸ§© The Solution: Word Embeddings  

Alright, if itâ€™s not a fixed number, maybe it can be some dynamically changing number â€” but how do we assign a number (or set of numbers) to a word in a way that reflects its meaning?  

ğŸ‘‰ The solution to this problem is **word embeddings**.  

Think of it as a wide **vector space** ğŸª, where each direction captures some underlying dimension â€” maybe gender, beauty, wealth, or anything else.  

Each word gets represented by a **high-dimensional vector**, with values along these various directions reflecting aspects of its meaning.  

âœ¨ For example, the word *king* might carry weight along the â€œroyaltyâ€ ğŸ‘‘ dimension, the â€œpowerâ€ âš”ï¸ dimension, and the â€œmaleâ€ ğŸš¹ dimension.  

---
